# Parallelise Datacat

[datacat]() is a port of [datafrog]() to D. After I achieved my goal of being at least equivalent in performans to the Rust implementation I turned my eye to parallelism. Could I improve performance even further by leveraging parallelism?

# Analyze

Before doing any optimization it is important to understand what part of your program that is the bottlenecks. To find this out for `datacat` I implemented some benchmarks for the high level API functions `join`, `antiJoin` and `map`. [See]().

This is implemented as a separate program that uses `datacat` as a library to make it possible to profile various aspects. The benchmark program writes the final results to a CSV file.

**Note**: The benchmark test suite were initially written as a unittest suite but this didn't work as expected when I wanted to compile with all optimizations turned on.

The CSV file contains coarse grained performance data. It is kind a boring to stare at numbers thus I wanted to visualize the data in some way. Visualization also helps when comparing a larger data set.

It wasn't obvious for me how to best visualize the data in the CSV file. I first tried to create pretty graphs with [gnuplot](http://www.gnuplot.info/). But I gave up in frustration. I never managed to achieve the results I wanted.

Then a bug tickled my mind. A friend had told me that R is good for plotting. But learning R for a little, simple plot? Creating a dependency on a whole new toolchain? Luckily for me there is a port of [ggplot2](https://www.statmethods.net/advgraphs/ggplot2.html) to D, [ggplotd](https://github.com/BlackEdder/ggplotd).

It [worked like a charm](https://github.com/joakim-brannstrom/datacat/blob/master/test/make_graph_from_benchmark.d)!

For anyone wanting to create plots I highly recommend the approach that ggplot have to creating graphs. In my opinion it is a lot easier than [matplotlib](https://matplotlib.org/) for Python.

## Profiling

Now with that out of the way I wanted to have detailed performance data from what parts of the program that are my bottlenecks. I thus turned to profiling.

This is easy to achieve with the [DMD]() compiler. It isn't the *best* tool for profiling but it is easy to use.
I added a new [build options]() to my `dub.sdl`.
```diff
buildType "utProf" {
    buildOptions "debugInfo" "profile" "optimize" "inline" "releaseMode"
}
```

And recompiled the benchmarks with this profile and executed it:
```sh
cd test
dub build -b utProf
./build/datacat_benchmark
```

This resulted in a `trace.log` and `trace.def` file. I have no idea what the `trace.def` file is. The `trace.log` is the one I want. But is it hard to read that file!

### Reading DMD Profile Log

The explanations here are what I managed to gather from the D forum. The words are not my own. Credit goes to those that wrote it.

Top part of a `trace.log`:Example of a `trace.log`:
```sh
------------------
    1    A
 1095    B
 1856    C
D      2952    122732995    3009972
```

D is called 2952 times. 1 of those calls comes from A. 1095 come from B.  1856 come from C. Note that 1+1095+1856 = 2952. (The A, B, C counts are called the "fan in".) The "fan out" is a list of counts of what D calls, and follows the line for D.

The total number of timer ticks spent in D is 3009972, excluding whatever D calls. The total number of timer ticks in D, including whatever D calls, is 122732995.

From this information you can not only determine where the time is being consumed, but *why*, as you know where the calls are coming from.  You can do this to determine the runtime relationships between functions, which can be used to set the link order (and this is output by the profiler in the form of a .def file).

Bottom part of a `trace.log`:Example of a `trace.log`:
```d
======== Timer Is 3579545 Ticks/Sec, Times are in Microsecs ========

  Num          Tree        Func        Per
  Calls        Time        Time        Call

1000000   390439368   265416744         265     pure nothrow @safe char[] std.array.array!(std.conv.toChars!(10, char, 1, int).toChars(int).Result).array(std.conv.toChars!(10, char, 1, int).toChars(int).Result)
1000000    83224994    83224994          83     pure nothrow ref @nogc @safe std.conv.toChars!(10, char, 1, int).toChars(int).Result std.conv.toChars!(10, char, 1, int).toChars(int).Result.__ctor(int)
23666670 1182768507    73190160           3     _Dmain
1000000   525732328    35191373          35     pure nothrow @trusted immutable(char)[] std.conv.toImpl!(immutable(char)[], int).toImpl(int, uint, std.ascii.LetterCase)
```

 - Num Calls is the number of calls made to that function
 - Tree Time is the total cumulative time spent in the function and its subfunctions
 - Func Time is the total time spent in the function only
 - Per Call is the average time spent in the function only

The functions are sorted by their Func Time. As we can see what took the most time was converting the range of chars to an array in order to store it in a string.

### Datacat `trace.log`

Luckily me (second time) there is an excellent analyzer [profdump]() that can transform a profile log to e.g. a dot graph. Unfortunately I ran into a bug. Somehow druntime is unable to deduce the timer frequency of my computer (too old?) and thus printed this line in the log.
`======== Timer frequency unknown, Times are in Megaticks ========`

`profdump` did not like that one. But no despair. We are programmers? We solve problems for a living? Yes we do. After a quick look at the stack trace I managed to locate the problem and solve it.

**Note**: The bug is fixed in v0.4.3.

Here are the commands I used in the end.
```sh
profdump -d -f -t 1.0 trace.log trace.dot
dot -Tsvg trace.dot -o trace.svg
```

I had to add a threshold (-t) because the figure became too large to read otherwise.

The end results is this pretty figure:
![performance bottlenecks](https://github.com/joakim-brannstrom/blog/master/assets/2018-07-24/profile_before_singlethread_optimization.svg)

## Summary

All this job to get here. Why? Listen to your elders. Everyone that I have heard of that is knowledgeable says to measure first then optimize.

It is two fold.
 * You need a way to **validate** that the changes actually improve the performance. This is why I have the coarse grained data and tools compare the gathered data.
 * You **must** understand what the bottlenecks are **before** doing any optimizations. This is why I have the profile data and visualization.

It honestly took me a couple of days to do the preparations. This is the nice thing though with doing it at your spare time. You can do it at your pace. No stress, no worries but still a clear goal to work towards. My brain works better under these conditions compared to a stressful deadline. Yours?

# Optimization of `join`

The graph clearly show that `join` is the culprit. Further down in the call stack it shows that the time is spent at sorting ranges.

I do not think that the sorting can be avoided. But can it be done in parallel? For primitive types this shouldn't be a problem. Phobos even have an example of [how to do this](https://dlang.org/phobos/std_parallelism.html#.task). Lets implement that!

A design to keep in mind is that this should be *optional*. Only parallelize if the user wants that behavior.

## Iteration

A user normally create variables via the `Iteration` type. My approach is to make `Iteration` into a template that take a parameter at compile time. This changes the behavior from single to multithreaded. This in turn affects all variables that are created via the `Iteration` type.

I hope that this will make it somewhat ergonomic for the user.

Lets see how it would be if we parallelize `changed`. From the call graph I do not expect this to have a major effect.
(Coding away)

After I finished the implementation of a parallel `changed` and fine tuned the API for creating parallel `Iteration` I came back to the benchmark test suite.
I implemented a new test, `perf_parallel_join`, where I create 10 variables that is continuously updated.

The results where very disappointing (and expected):
```sh
Running perf_parallel_join
2018-07-24T23:24:16.993:package.d:perf_parallel_join:75 datacat_test.benchmark.perf_parallel_join 75: lowest(4 secs, 262 ms, 494 μs, and 1 hnsec) total(43 secs, 32 ms, 47 μs, and 4 hnsecs)
2018-07-24T23:25:00.048:package.d:perf_parallel_join:78 datacat_test.benchmark.perf_parallel_join 78: lowest(4 secs, 274 ms, 481 μs, and 4 hnsecs) total(43 secs, 55 ms, 478 μs, and 2 hnsecs)
```

No change!
This is why it is so important to have a test suite to **validate** the code changes. This means that the changes are ineffective. They only complicate the implementation. I reverted back `changed` to the single threaded, simple implementation.

But the good thing is that we now have the infrastructure to create parallel `Relation`s! Lets do that. Lets implement that parallel sort. That surely should result in some markedly improvements.

Where do I add the parallel sort? In each datalog function such as `join`, `antiJoin` and `map`? No that spreads out. The crucial part is the sort in Relation. By letting the ThreadStrategy propagate to a function that take the thread strategy it reduced the code affected from three functions to one.

This do require some changes to Relation though. It has to be *aware* of the *sortedness* of the input. I looked into how [phobos]() do this for the different sort functions defined in [std.algorithm.sorting](). If the input range can be implicitly converted to a SortedRange, defined in [std.range](), it means that the input do not need to be sorted. Use as is. This is exactly what I want `Relation` to do in its constructor.

Where do I then add the parallel sort?
It seems like `Relation.from` is a good candidate. Lets extend it with an implementation for `ThreadStrategy.parallel`.

After some study of `Relation.from` and thinking of the use cases for it I concluded that "no". This is not the right place to add the parallel sort. What about the constructor itself?

I do not think it is a good idea to make a new type that is dependent on the `ThreadStrategy`. To avoid this I extended the constructor instead of the type `Relation` itself. This may be the wrong decision because that mean that `Relation.merge` is always single threaded. Lets see later on what the profiling data tells us.

There is a parallel sort example in [std.parallelism](). I do not see any reason to try to duplicate the effort there so I chose to copy it as is into the constructor.

So far we have now made `Iteration`, `Variable` and `Relation` aware of `ThreadStrategy`. The last peace of the puzzle is to make `datacat.join.join` create sorted `Relation`s using the thread strategy of the `Variable`. This where achieved by adding a new template parameter, `ThreadStrategy TS`, that propagates `TS` to the contructor of `Relation`.

What is the end result when running the coarse grained benchmarks?
```sh
2018-07-25T14:21:50.879:package.d:perf_parallel_join:75 datacat_test.benchmark.perf_parallel_join 75: lowest(414 ms, 95 μs, and 1 hnsec) total(4 secs, 249 ms, 611 μs, and 4 hnsecs)
2018-07-25T14:21:54.793:package.d:perf_parallel_join:78 datacat_test.benchmark.perf_parallel_join 78: lowest(353 ms, 364 μs, and 8 hnsecs) total(3 secs, 913 ms, 818 μs, and 2 hnsecs)
```

Nice! That is a 15% improvement on a small dataset.
Lets see what the results are when running the graspan1 dataset for Apache.

| Single   | Multi |
|----------|-------|
| 3s       | 6s    |

What! What just happend? It got super slow?

This my folk is why it is so important to have a test suite.

No, multithreading is not an answer to everything. This is definitly not the answer in this case.

This is the end of this blog post. In the future I'll look into other ways of improving the performance and analyze "why" this got slower than the single threaded.
