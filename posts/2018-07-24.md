# Parallelise Datacat

[datacat]() is a port of [datafrog]() to D. After I achieved slightly better performance than the Rust implementation I turned my eye to parallelism. Could I improve performance even further by leveraging parallelism?

D do have excellent support for parallelism in the [stdlib]() in the form of [std.parallelism]().

# Analyze

Before doing any optimization it is important to understand what part of your program that is the bottlenecks. To find this out for `datacat` I implemented some benchmarks for the high level API functions `join`, `antiJoin` and `map`. [See]().

This is implemented as a separate program that uses `datacat` as a library to make it possible to profile various aspects. The benchmark program writes the final results to a CSV file.

**Note**: The benchmark test suite were initially written as a unittest suite but this didn't work as expected when I wanted to compile with all optimizations turned on.

The CSV file contains coarse grained performance data. It is kind a boring to stare at numbers thus I wanted to visualize the data in some way. Visualization also helps when comparing a larger data set.

It wasn't obvious for me how to best visualize the data in the CSV file. I first tried to create pretty graphs with [gnuplot](http://www.gnuplot.info/). But I gave up in frustration. I never managed to achieve the results I wanted.

Then a bug tickled my mind. A friend had told me that R is good for plotting. But learning R for a little, simple plot? Creating a dependency on a whole new toolchain? Luckily for me there is a port of [ggplot2](https://www.statmethods.net/advgraphs/ggplot2.html) to D, [ggplotd](https://github.com/BlackEdder/ggplotd).

It [worked like a charm](https://github.com/joakim-brannstrom/datacat/blob/master/test/make_graph_from_benchmark.d)!

For anyone wanting to create plots I highly recommend the approach that ggplot have to creating graphs. In my opinion it is a lot easier than [matplotlib](https://matplotlib.org/) for Python.

## Profiling

Now with that out of the way I wanted to have detailed performance data from what parts of the program that are my bottlenecks. I thus turned to profiling.

This is easy to achieve with the [DMD]() compiler. I added a new [build options]() to my `dub.sdl`.
```diff
buildType "utProf" {
    buildOptions "debugInfo" "profile" "optimize" "inline" "releaseMode"
}
```

and recompiled the benchmarks with this profile and executed it:
```sh
cd test
dub build -b utProf
./build/datacat_benchmark
```

This resulted in a `trace.log` and `trace.def` file. I have no idea what the `trace.def` file is. But the `trace.log` is the one I want.

But is it hard to read that file! Luckily me (second time) there is an excellent analyzer [profdump]() that can transform a profile log to e.g. a dot graph.

Unfortunately I ran into a bug. Somehow druntime is unable to deduce the timer frequency of my computer (too old?) and thus printed this line in the log.
`======== Timer frequency unknown, Times are in Megaticks ========`

`profdump` did not like that one. But no despair. We are programmers? We solve problems for a living? Yes we do. After a quick look at the stack trace I understood the problem and where able to solve it.

The end results is this pretty figure:
![performance bottlenecks](https://github.com/joakim-brannstrom/blog/master/assets/2018-07-24/profile_before_singlethread_optimization.svg)

**Note**: The bug in `profdump` is fixed in v0.4.3

## Summary

All this job to get here. Why? Listen to your elders. Everyone that I have heard of that is knowledgeable says to measure first then optimize.

It is two fold.
 * You need a way to **validate** that the changes actually improve the performance. This is why I in this case have the coarse grained data.
 * You **must** understand what the bottlenecks are **before** doing any optimizations. This is why I have the profile data and visualization.

# Optimization of `join`

The graph clearly show that `join` is the culprit. Further down in the call stack it shows that the time is spent at sorting ranges.

I do not think that the sorting can be avoided. But can it be done in parallel? For primitive types this shouldn't be a problem. Phobos even have an example of [how to do this](https://dlang.org/phobos/std_parallelism.html#.task). Lets implement that!

A design to keep in mind is that this should be *optional*. Only parallelize if the user wants that behavior.

## Iteration

A user normally create variables via the `Iteration` type. My approach is to make `Iteration` into a template that take a parameter at compile time. This changes the behavior from single to multithreaded. This in turn affects all variables that are created via the `Iteration` type.

I hope that this will make it somewhat ergonomic for the user.

Lets see how it would be if we parallelize `changed`. From the call graph I do not expect this to have a major effect.
(Coding away)

After I finished the implementation of a parallel `changed` and fine tuned the API for creating parallel `Iteration` I came back to the benchmark test suite.
I implemented a new test, `perf_parallel_join`, where I create 10 variables that is continuously updated.

The results where very disappointed (and expected):
```sh
Running perf_parallel_join
2018-07-24T23:24:16.993:package.d:perf_parallel_join:75 datacat_test.benchmark.perf_parallel_join 75: lowest(4 secs, 262 ms, 494 μs, and 1 hnsec) total(43 secs, 32 ms, 47 μs, and 4 hnsecs)
2018-07-24T23:25:00.048:package.d:perf_parallel_join:78 datacat_test.benchmark.perf_parallel_join 78: lowest(4 secs, 274 ms, 481 μs, and 4 hnsecs) total(43 secs, 55 ms, 478 μs, and 2 hnsecs)
```

No change!
This is why it is so important to have a test suite to **verify** the code changes. This means that the changes are ineffective. They only complicate the implementation. I reverted back `changed` to the single threaded, simple implementation.

But the good thing is that we now have the infrastructure to create parallel `Relation`s! Lets do that. Lets implement that parallel sort. That surely should result in some markedly improvements.
